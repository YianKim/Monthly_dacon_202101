{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "분석python.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "TIG8ZAMmyiCw",
        "RwgQm03Gyk03"
      ]
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSWXkgz7EvkE"
      },
      "source": [
        "colab : 드라이브 마운트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCGl4XsyPjTG"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhSggYe9PedK"
      },
      "source": [
        "import tensorflow as tf\n",
        "from keras import optimizers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, BatchNormalization, Conv1D, MaxPooling1D, Dropout, Flatten, Activation\n",
        "from sklearn.model_selection import KFold"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDg4kfqoPedI"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import *\n",
        "from sklearn import svm\n",
        "from scipy.stats import randint, uniform\n",
        "from sklearn.feature_selection import SelectKBest, f_regression, f_classif\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.ensemble import  GradientBoostingClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.metrics import classification_report,roc_auc_score\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "train=pd.read_csv('/content/drive/MyDrive/월간데이콘11/train_features.csv')\n",
        "train_labels=pd.read_csv('/content/drive/MyDrive/월간데이콘11/train_labels.csv')\n",
        "test=pd.read_csv('/content/drive/MyDrive/월간데이콘11/test_features.csv')\n",
        "submission=pd.read_csv('/content/drive/MyDrive/월간데이콘11/sample_submission.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIG8ZAMmyiCw"
      },
      "source": [
        "# 전처리\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcVxpTWsE1kS"
      },
      "source": [
        "총 가속도 합을 변수로 추가하였습니다.\r\n",
        "\r\n",
        "나름의 효과가 있어 긍정적으로 모델에 반영하였습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXGegK8bPedL"
      },
      "source": [
        "train[\"acc_total\"]=(train[\"acc_x\"]**2+train[\"acc_y\"]**2+train[\"acc_z\"]**2)**0.5\n",
        "test[\"acc_total\"]=(test[\"acc_x\"]**2+test[\"acc_y\"]**2+test[\"acc_z\"]**2)**0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQxOGwBuFE7x"
      },
      "source": [
        "스케일링을 조금 더 쉽게 하기 위해서 임시로 데이터셋을 잘랐습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wXlSxELPedL"
      },
      "source": [
        "acc=train[[\"acc_x\",\"acc_y\",\"acc_z\"]]\n",
        "gy=train[[\"gy_x\",\"gy_y\",\"gy_z\"]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hx4HVB2JPedM"
      },
      "source": [
        "acto=train[\"acc_total\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9frSSRLFN5a"
      },
      "source": [
        "스케일링을 하는 것이 결과면에서 조금 더 나은 것 같아 진행하였습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTvu9OkfPedM"
      },
      "source": [
        "train[\"acc_x_norm\"]=(train[\"acc_x\"]-np.nanmean(acc))/np.nanstd(acc)\n",
        "train[\"acc_y_norm\"]=(train[\"acc_y\"]-np.nanmean(acc))/np.nanstd(acc)\n",
        "train[\"acc_z_norm\"]=(train[\"acc_z\"]-np.nanmean(acc))/np.nanstd(acc)\n",
        "\n",
        "train[\"gy_x_norm\"]=(train[\"gy_x\"]-np.nanmean(gy))/np.nanstd(gy)\n",
        "train[\"gy_y_norm\"]=(train[\"gy_y\"]-np.nanmean(gy))/np.nanstd(gy)\n",
        "train[\"gy_z_norm\"]=(train[\"gy_z\"]-np.nanmean(gy))/np.nanstd(gy)\n",
        "\n",
        "train[\"acc_total_norm\"]=(train[\"acc_total\"]-mean(acto))/std(acto)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpbXH1-FPedN"
      },
      "source": [
        "test[\"acc_x_norm\"]=(test[\"acc_x\"]-np.nanmean(acc))/np.nanstd(acc)\n",
        "test[\"acc_y_norm\"]=(test[\"acc_y\"]-np.nanmean(acc))/np.nanstd(acc)\n",
        "test[\"acc_z_norm\"]=(test[\"acc_z\"]-np.nanmean(acc))/np.nanstd(acc)\n",
        "\n",
        "test[\"gy_x_norm\"]=(test[\"gy_x\"]-np.nanmean(gy))/np.nanstd(gy)\n",
        "test[\"gy_y_norm\"]=(test[\"gy_y\"]-np.nanmean(gy))/np.nanstd(gy)\n",
        "test[\"gy_z_norm\"]=(test[\"gy_z\"]-np.nanmean(gy))/np.nanstd(gy)\n",
        "\n",
        "test[\"acc_total_norm\"]=(test[\"acc_total\"]-mean(acto))/std(acto)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqT1BxYbFWD5"
      },
      "source": [
        "원래의 데이터는 드랍하였습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChdckHuQPedN"
      },
      "source": [
        "train=train.drop(columns=[\"acc_x\", \"acc_y\", \"acc_z\",\n",
        "                   \"gy_x\", \"gy_y\", \"gy_z\", \"acc_total\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "na8JQC0ePedN"
      },
      "source": [
        "test=test.drop(columns=[\"acc_x\", \"acc_y\", \"acc_z\",\n",
        "                   \"gy_x\", \"gy_y\", \"gy_z\", \"acc_total\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwgQm03Gyk03"
      },
      "source": [
        "# data generate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdZ-Zwg3Faak"
      },
      "source": [
        "데이터 불균형이 있긴 한데, 불균형 정도가 좀 심한 것 같아서 샘플링은 부적절하다 느꼈습니다.\r\n",
        "\r\n",
        "차라리 데이터를 임의의 시간에서 잘라 붙여서, 데이터 자체의 크기를 늘려보았습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoDvikr3PedO"
      },
      "source": [
        "# data #\n",
        "new_train=train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImbScm0sPedO"
      },
      "source": [
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "for i in tqdm(range(3125), desc=\"tqdm example\", mininterval=0.01):\n",
        "    temp=train[train[\"id\"]==i]\n",
        "    aa=random.randrange(1,600)\n",
        "    temp2=temp[aa:600].append(temp[0:aa])\n",
        "    new_train=new_train.append(temp2)\n",
        "    \n",
        "for i in tqdm(range(3125), desc=\"tqdm example\", mininterval=0.01):\n",
        "    temp=train[train[\"id\"]==i]\n",
        "    aa=random.randrange(1,600)\n",
        "    temp2=temp[aa:600].append(temp[0:aa])\n",
        "    new_train=new_train.append(temp2)\n",
        "    \n",
        "for i in tqdm(range(3125), desc=\"tqdm example\", mininterval=0.01):\n",
        "    temp=train[train[\"id\"]==i]\n",
        "    aa=random.randrange(1,600)\n",
        "    temp2=temp[aa:600].append(temp[0:aa])\n",
        "    new_train=new_train.append(temp2)\n",
        "    \n",
        "for i in tqdm(range(3125), desc=\"tqdm example\", mininterval=0.01):\n",
        "    temp=train[train[\"id\"]==i]\n",
        "    aa=random.randrange(1,600)\n",
        "    temp2=temp[aa:600].append(temp[0:aa])\n",
        "    new_train=new_train.append(temp2)\n",
        "    \n",
        "for i in tqdm(range(3125), desc=\"tqdm example\", mininterval=0.01):\n",
        "    temp=train[train[\"id\"]==i]\n",
        "    aa=random.randrange(1,600)\n",
        "    temp2=temp[aa:600].append(temp[0:aa])\n",
        "    new_train=new_train.append(temp2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFmXi9DWPedP"
      },
      "source": [
        "new_y=train_labels['label'].append(train_labels['label'])\n",
        "new_y=new_y.append(train_labels['label'])\n",
        "new_y=new_y.append(train_labels['label'])\n",
        "new_y=new_y.append(train_labels['label'])\n",
        "new_y=new_y.append(train_labels['label'])\n",
        "\n",
        "X=tf.reshape(np.array(new_train.iloc[:,2:]),[-1, 600, 7])\n",
        "X.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMLBTAJUPedP"
      },
      "source": [
        "y = tf.keras.utils.to_categorical(new_y) \n",
        "y.shape\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXviETLbynsq"
      },
      "source": [
        "# 1d cnn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrgUffMACerd"
      },
      "source": [
        "test_X=tf.reshape(np.array(test.iloc[:,2:]),[-1, 600, 7])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-QH01ErFxrp"
      },
      "source": [
        "모델링은 1d cnn을 활용했습니다.\r\n",
        "\r\n",
        "논문을 참고했는데, 보통 cnn을 많이 쓰길래 시도해봤더니 1d cnn이 가장 좋은 결과가 나왔습니다.\r\n",
        "\r\n",
        "깊은 모델, 얕은 모델, 그리고 그 중간. 이 세가지 cnn을 만들어 soft voting을 하면 결과가 좋게 나왔습니다.\r\n",
        "\r\n",
        "repeat을 주는 대신 drop을 0.5로 설정하여 과적합을 최대한 피하고자 했습니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64k75b6igami"
      },
      "source": [
        "prediction=0\r\n",
        "folds=12\r\n",
        "repeats=15\r\n",
        "epochsize=40\r\n",
        "drop=0.5\r\n",
        "kfold = KFold(n_splits=folds, shuffle=True)\r\n",
        "\r\n",
        "for k in range(repeats):\r\n",
        "  for train_ind, test_ind in kfold.split(X, y):\r\n",
        "    model = Sequential()\r\n",
        "    model.add(Conv1D (kernel_size=60, filters=256, strides=3, padding='valid',\r\n",
        "                    kernel_initializer='he_uniform', input_shape=[600,7],\r\n",
        "                    activation='relu'))\r\n",
        "    model.add(MaxPooling1D(1, padding = 'valid'))\r\n",
        "\r\n",
        "    model.add(Conv1D(kernel_size=60, filters=128, activation='relu'))\r\n",
        "    model.add(MaxPooling1D(1, padding = 'valid'))\r\n",
        "\r\n",
        "    model.add(Conv1D(kernel_size=60, filters=64, activation='relu'))\r\n",
        "    model.add(MaxPooling1D(1, padding = 'valid'))\r\n",
        "\r\n",
        "    model.add(BatchNormalization())\r\n",
        "\r\n",
        "    model.add(Dropout(drop))\r\n",
        "    model.add(Flatten())\r\n",
        "    model.add(Dense(1024, activation='relu'))\r\n",
        "    model.add(BatchNormalization())\r\n",
        "\r\n",
        "    model.add(Dropout(drop))\r\n",
        "    model.add(Dense(1024, activation='relu'))\r\n",
        "    model.add(BatchNormalization())\r\n",
        "\r\n",
        "    model.add(Dropout(drop))\r\n",
        "    model.add(Dense(1024, activation='relu'))\r\n",
        "    model.add(BatchNormalization())\r\n",
        "\r\n",
        "\r\n",
        "    model.add(Dropout(drop))\r\n",
        "    model.add(Dense(61))\r\n",
        "    model.add(Activation('softmax'))\r\n",
        "\r\n",
        "    adam = optimizers.Adam(lr=0.0001)\r\n",
        "\r\n",
        "    model.compile(loss='categorical_crossentropy',\r\n",
        "                  optimizer=adam,\r\n",
        "                  metrics=['accuracy'])\r\n",
        "    \r\n",
        "    model.fit(np.array(X)[train_ind], y[train_ind], epochs=epochsize, batch_size=256,\r\n",
        "              validation_data=(np.array(X)[test_ind], y[test_ind]))\r\n",
        "\r\n",
        "    prediction += (model.predict(test_X)/(folds*repeats))\r\n",
        "\r\n",
        "submission.iloc[:,1:]=prediction\r\n",
        "submission.to_csv('/content/drive/MyDrive/월간데이콘11/py_answer1.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_PCZi0slrIg"
      },
      "source": [
        "prediction=0\r\n",
        "folds=12\r\n",
        "repeats=15\r\n",
        "epochsize=40\r\n",
        "drop=0.5\r\n",
        "kfold = KFold(n_splits=folds, shuffle=True)\r\n",
        "\r\n",
        "for k in range(repeats):\r\n",
        "  for train_ind, test_ind in kfold.split(X, y):\r\n",
        "    model = Sequential()\r\n",
        "    model.add(Conv1D (kernel_size=60, filters=256, strides=3, padding='valid',\r\n",
        "                     kernel_initializer='he_uniform', input_shape=[600,7],\r\n",
        "                     activation='relu'))\r\n",
        "    model.add(MaxPooling1D(1, padding = 'valid'))\r\n",
        "\r\n",
        "    model.add(Conv1D(kernel_size=60, filters=128, activation='relu'))\r\n",
        "    model.add(MaxPooling1D(1, padding = 'valid'))\r\n",
        "\r\n",
        "    model.add(BatchNormalization())\r\n",
        "\r\n",
        "    model.add(Dropout(drop))\r\n",
        "    model.add(Flatten())\r\n",
        "    model.add(Dense(1024, activation='relu'))\r\n",
        "    model.add(BatchNormalization())\r\n",
        "\r\n",
        "    model.add(Dropout(drop))\r\n",
        "    model.add(Dense(61))\r\n",
        "    model.add(Activation('softmax'))\r\n",
        "\r\n",
        "    adam = optimizers.Adam(lr=0.0001)\r\n",
        "\r\n",
        "    model.compile(loss='categorical_crossentropy',\r\n",
        "                  optimizer=adam,\r\n",
        "                  metrics=['accuracy'])\r\n",
        "  \r\n",
        "    model.fit(np.array(X)[train_ind], y[train_ind], epochs=epochsize, batch_size=64,\r\n",
        "              validation_data=(np.array(X)[test_ind], y[test_ind]))\r\n",
        "\r\n",
        "    prediction += (model.predict(test_X)/(folds*repeats))\r\n",
        "\r\n",
        "submission.iloc[:,1:]=prediction\r\n",
        "submission.to_csv('/content/drive/MyDrive/월간데이콘11/py_answer2.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xd26I5F9xCD"
      },
      "source": [
        "prediction=0\r\n",
        "folds=12\r\n",
        "repeats=15\r\n",
        "epochsize=40\r\n",
        "drop=0.5\r\n",
        "kfold = KFold(n_splits=folds, shuffle=True)\r\n",
        "\r\n",
        "for k in range(repeats):\r\n",
        "  for train_ind, test_ind in kfold.split(X, y):\r\n",
        "    model = Sequential()\r\n",
        "    model.add(Conv1D (kernel_size=60, filters=256, strides=3, padding='valid',\r\n",
        "                     kernel_initializer='he_uniform', input_shape=[600,7],\r\n",
        "                     activation='relu'))\r\n",
        "    model.add(MaxPooling1D(1, padding = 'valid'))\r\n",
        "    \r\n",
        "    model.add(Conv1D(kernel_size=60, filters=128, activation='relu'))\r\n",
        "    model.add(MaxPooling1D(1, padding = 'valid'))\r\n",
        "    \r\n",
        "    model.add(Conv1D(kernel_size=60, filters=64, activation='relu'))\r\n",
        "    model.add(MaxPooling1D(1, padding = 'valid'))\r\n",
        "\r\n",
        "    model.add(Conv1D(kernel_size=60, filters=32, activation='relu'))\r\n",
        "    model.add(MaxPooling1D(1, padding = 'valid'))\r\n",
        "\r\n",
        "    model.add(BatchNormalization())\r\n",
        "\r\n",
        "    model.add(Dropout(drop))\r\n",
        "    model.add(Flatten())\r\n",
        "    model.add(Dense(1024, activation='relu'))\r\n",
        "    model.add(BatchNormalization())\r\n",
        "\r\n",
        "    model.add(Dropout(drop))\r\n",
        "    model.add(Dense(1024, activation='relu'))\r\n",
        "    model.add(BatchNormalization())\r\n",
        "\r\n",
        "    model.add(Dropout(drop))\r\n",
        "    model.add(Dense(61))\r\n",
        "    model.add(Activation('softmax'))\r\n",
        "\r\n",
        "    adam = optimizers.Adam(lr=0.0001)\r\n",
        "\r\n",
        "    model.compile(loss='categorical_crossentropy',\r\n",
        "                  optimizer=adam,\r\n",
        "                  metrics=['accuracy'])\r\n",
        "  \r\n",
        "    model.fit(np.array(X)[train_ind], y[train_ind], epochs=epochsize, batch_size=64,\r\n",
        "              validation_data=(np.array(X)[test_ind], y[test_ind]))\r\n",
        "\r\n",
        "    prediction += (model.predict(test_X)/(folds*repeats))\r\n",
        "\r\n",
        "submission.iloc[:,1:]=prediction\r\n",
        "submission.to_csv('/content/drive/MyDrive/월간데이콘11/py_answer3.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amQC18M-Cj5K"
      },
      "source": [
        "# Softvoting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uD7mTM5Cjny"
      },
      "source": [
        "model1ans=pd.read_csv('/content/drive/MyDrive/월간데이콘11/py_answer1.csv')\r\n",
        "model2ans=pd.read_csv('/content/drive/MyDrive/월간데이콘11/py_answer2.csv')\r\n",
        "model3ans=pd.read_csv('/content/drive/MyDrive/월간데이콘11/py_answer3.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5CSxhviDsi2"
      },
      "source": [
        "final_sv=(model1ans+model2ans+model3ans)/3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZErilNgEPoN"
      },
      "source": [
        "# final_sv.to_csv('/content/drive/MyDrive/월간데이콘11/py_new4.csv', index=False)\r\n",
        "\r\n",
        "final_sv.to_csv('/content/drive/MyDrive/월간데이콘11/py_final_answer.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}